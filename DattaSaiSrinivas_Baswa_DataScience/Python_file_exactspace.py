# -*- coding: utf-8 -*-
"""ExactSpace.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-6CVu13DJc58F_4x0iHkHpao2bfhrZoF

# ExactSpace Data Science Internship Assessment
"""

#Importing necessary libraries for further Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df= pd.read_excel("/content/data.xlsx") #Reading the given dataset

df.head() # Retrieving the first five rows of the DataFrame

df.shape #Shape of the dataset

df.loc[104680] #checking the random row values

"""# Data Preprocessing"""

df.dtypes #checking the datatypes of the each column

type(df.iloc[0,1]), type(df.iloc[377718,4]) #Checking whether all the values(also numerical/float) are Object or Not

unique= df[df['Cyclone_Inlet_Gas_Temp'].apply(lambda x: isinstance(x, str))] #Retreiving the rows of Cyclone_Inlet_Gas_Temp consisting the Object datatype

unique

counts = unique['Cyclone_Inlet_Gas_Temp'].value_counts() #Count of the unique object datatypes in 'Cyclone_Inlet_Gas_Temp'
counts

unique['Cyclone_Inlet_Gas_Temp'].unique()

unique['Cyclone_Outlet_Gas_draft'].unique()

material_unique= df[df['Cyclone_Material_Temp'].apply(lambda x: isinstance(x, str))] #Retreiving the rows of Cyclone_Material_Temp consisting the Object datatype

material_unique

counts= material_unique['Cyclone_Material_Temp'].value_counts() #Count of the unique object datatypes in 'Cyclone_Material_Temp'
counts

outlet_unique = df[df['Cyclone_Outlet_Gas_draft'].apply(lambda x: isinstance(x, str))]  #Retreiving the rows of 'Cyclone_Outlet_Gas_draft' consisting the Object datatype

outlet_unique

outlet_unique['Cyclone_Outlet_Gas_draft'].value_counts() #Count of the unique object datatypes in 'Cyclone_Outlet_Gas_draft'

cone_unique= df[df['Cyclone_cone_draft'].apply(lambda x: isinstance(x, str))] #Retreiving the rows of 'Cyclone_cone_draft' consisting the Object datatype
cone_unique

cone_unique['Cyclone_cone_draft'].value_counts() #Count of the unique object datatypes in 'Cyclone_cone_draft'

gas_unique= df[df['Cyclone_Gas_Outlet_Temp'].apply(lambda x: isinstance(x, str))] #Retreiving the rows of 'Cyclone_Gas_Outlet_Temp' consisting the Object datatype
gas_unique

cone_unique['Cyclone_Gas_Outlet_Temp'].value_counts() #Count of the unique object datatypes in 'Cyclone_cone_draft'

inlet_unique= df[df['Cyclone_Inlet_Draft'].apply(lambda x: isinstance(x, str))]  #Retreiving the rows of 'Cyclone_Inlet_Draft' consisting the Object datatype
inlet_unique

inlet_unique['Cyclone_Inlet_Draft'].value_counts() #Count of the unique object datatypes in 'Cyclone_Inlet_Draft'

"""* More than 700 "Not Connect" values, 470 "I/O Timeout" values and 100+ "Configure" values were there in the dataset.
* We have to handle them by replacing by an appropriate values.
* Deleting the rows that consists of "Comm Fail" as it can't impact much in the dataset.
"""

df.shape #Checking the shape of the dataframe before data preprocessing

#Deleting the rows that consists of "Comm Fail"
df1 = df[~df.isin(['Comm Fail']).any(axis=1)]

df1.reset_index(drop=True, inplace=True) # Reset the index if needed

df1.shape #shape after deleting the two rows

"""Deleted two rows which are not much impactable in dataset."""

df1.head()

# Create a copy of the original dataframe
df_copy = df.copy()

# Convert non-numeric values to NaN in the non-datetime columns of the copy
numeric_cols = df_copy.select_dtypes(exclude='datetime').columns
df_copy[numeric_cols] = df_copy[numeric_cols].apply(pd.to_numeric, errors='coerce')
df_copy.reset_index(drop=True, inplace=True)

df_copy.head()

df_copy.loc[2471] #Checking the row values for a random number '2471'

"""**Observation-1**: All the parameters are NaN values except "Time"."""

df_copy.loc[375585] #Checking the row values for a random number '375585'

"""**Observation-2**: All the parameters having a value, except 'Cyclone_Material_Temp'

## Data Visualization for Preprocessing
"""

mask = pd.to_numeric(df['Cyclone_Inlet_Gas_Temp'], errors='coerce').notnull() #Plotting the graph for 'Cyclone_Inlet_Gas_Temp' feature against 'time'
filtered_df = df.loc[mask]

# Plot the graph
plt.plot(filtered_df['time'], pd.to_numeric(filtered_df['Cyclone_Inlet_Gas_Temp']))
plt.xlabel('Time')
plt.ylabel('Cyclone_Inlet_Gas_Temp')
plt.title('Cyclone_Inlet_Gas_Temp over Time')
plt.xticks(rotation=45)
plt.show()

df['Cyclone_Inlet_Gas_Temp'] = pd.to_numeric(df['Cyclone_Inlet_Gas_Temp'], errors='coerce') #Plotting the graph for 'Cyclone_Inlet_Gas_Temp' feature against 'time'

plt.figure(figsize=(15, 10))

# Plot the graph with NaN values as gaps
plt.plot(df['time'], df['Cyclone_Inlet_Gas_Temp'])
plt.xlabel('Time')
plt.ylabel('Cyclone_Inlet_Gas_Temp')
plt.title('Cyclone_Inlet_Gas_Temp over Time')
plt.xticks(rotation=45)

# Remove the NaN values from the plot
plt.plot(df['time'], df['Cyclone_Inlet_Gas_Temp'], 'o', markersize=2, color='red')

plt.show()

df['Cyclone_Material_Temp'] = pd.to_numeric(df['Cyclone_Material_Temp'], errors='coerce') #Plotting the graph for 'Cyclone_Material_Temp' feature against 'time'

plt.figure(figsize=(15, 10))

# Plot the graph with NaN values as gaps
plt.plot(df['time'], df['Cyclone_Material_Temp'])
plt.xlabel('Time')
plt.ylabel('Cyclone_Material_Temp')
plt.title('Cyclone_Material_Temp over Time')
plt.xticks(rotation=45)

# Remove the NaN values from the plot
plt.plot(df['time'], df['Cyclone_Material_Temp'], '*', markersize=2, color='red')

plt.show()

"""# Applying "Interpolation" method for replacing the anomalies"""

columns_to_interpolate = ['Cyclone_Inlet_Gas_Temp', 'Cyclone_Material_Temp', 'Cyclone_Outlet_Gas_draft',
                          'Cyclone_cone_draft', 'Cyclone_Gas_Outlet_Temp', 'Cyclone_Inlet_Draft']
df_interpolated = df_copy.copy() #Copying the 'df_copy' into 'df_interpolated'

# Apply interpolation to the desired columns
df_interpolated[columns_to_interpolate] = df_interpolated[columns_to_interpolate].interpolate(method='linear')

# Verifing the dataset after interpolation
df_interpolated.head()

df_interpolated.loc[2471] #Checking the row '2471' whether all the features were replaced or not

"""**Observation-3**: As in Observation-1 all the features which were NaN were replaced with Interpolated values."""

df_interpolated.loc[375585] #Checking the row '375585' whether all the features were replaced or not

"""**Observation-4**: As in Observation-2 Cyclone_Material_Temp was NaN is replaced with Interpolated value."""

df_interpolated.loc[322817]

df_interpolated.head()

df_interpolated.shape #Checking the shape of the dataset after Interpolation Method

df_interpolated.to_csv('preprocessed_data.csv', index=True) #Exporting the Preprocessed data for further Analysis

#Importing Libraries for further Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

data= pd.read_csv("preprocessed_data.csv")
data = data.drop(data.columns[0], axis=1)

data.head() #Retreiving the first four rows after preprocessing the data

data['time'] = pd.to_datetime(data['time']) #Checking whether any anomalies(string or object) are present or not
data.dtypes

"""**Observation-5**: No datatype is present other than Numerical and Datetime for further analysis.

# Feature Engineering
"""

data1= data.copy() #Copying the 'data' dataframe to 'data1'
data1.head()

"""## Adding day, month and year to obtain further insights"""

data1['day']= data1['time'].dt.day
data1['month']= data1['time'].dt.month
data1['year']= data1['time'].dt.year

data1.head()

"""## Adding 'season' feature"""

data1['time'] = pd.to_datetime(data1['time'])

# Define a function to map months to seasons
def get_season(month):
    if month in [12, 1, 2]: #Months like December, January and February are the Winter Season
        return 'Winter'
    elif month in [6, 7, 8]: #Months like June, July and August are the Monsoon Season
        return 'Rainy'
    else:
        return 'Summer'      #Rest of the months I am considering as Summer

# Apply the function to create the 'season' column
data1['season'] = data1['time'].dt.month.apply(get_season)

data1.head()

"""## Adding 'Week' number for observation on weekly basis"""

data1['weekday'] = data1['time'].dt.day_name()

data1.head()

data1.columns

"""## Cyclic Features"""

data1['hour_of_day'] = data1['time'].dt.hour
data1['day_of_year'] = data1['time'].dt.dayofyear

data1.head()

data1['date'] = data1['time'].dt.date

data1.head()

data1['Time'] = data1['time'].dt.strftime('%H:%M')

data1.head()

data1.columns

"""## Adding "time_category" column consists (morning, afternoon, evening and night)"""

def get_time_category(time):
    hour = time.hour
    if 5 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 17:
        return 'Afternoon'
    elif 17 <= hour < 21:
        return 'Evening'
    else:
        return 'Night'

# Convert 'time' column to datetime type
data['time'] = pd.to_datetime(data['time'])

# Apply the function to create the 'time_category' column
data['time_category'] = data['time'].apply(get_time_category)

data.head()

"""#Data Visualization

## Outlier Detection
"""

data.head()

data.columns

import seaborn as sns
subset_data = data[['Cyclone_Inlet_Gas_Temp', 'Cyclone_Material_Temp',
                   'Cyclone_Outlet_Gas_draft', 'Cyclone_cone_draft',
                   'Cyclone_Gas_Outlet_Temp', 'Cyclone_Inlet_Draft']]

# Reshape the DataFrame for plotting
melted_data = pd.melt(subset_data)

# Set the figure size
plt.figure(figsize=(10, 6))

# Create boxplots using seaborn
sns.boxplot(x='variable', y='value', data=melted_data)

# Set labels and title
plt.xlabel('Columns')
plt.ylabel('Values')
plt.title('Boxplots for Cyclone Data')

# Rotate x-axis labels for better visibility
plt.xticks(rotation=90)

# Show the plot
plt.show()

subset_data.describe().round(2)

for column in subset_data.columns:
    # Calculate the lower and upper whiskers
    whiskers = data[column].describe()[['25%', '75%']].values
    iqr = whiskers[1] - whiskers[0]
    lower_whisker = whiskers[0] - 1.5 * iqr
    upper_whisker = whiskers[1] + 1.5 * iqr

    # Count the values outside minima and maxima
    count_below_minima = (data[column] < lower_whisker).sum()
    count_above_maxima = (data[column] > upper_whisker).sum()

    print("Column:", column)
    print("Values below minima:", count_below_minima)
    print("Values above maxima:", count_above_maxima)
    print()

"""#Obtaining Insights

### Power BI was utilized to visualize the most crucial insights, as it proved to be a versatile and efficient tool compared to Python. Its capabilities enabled us to obtain a comprehensive and detailed understanding of the data, revealing numerous valuable insights. The visualizations created in Power BI facilitated a more thorough analysis and interpretation of the data, leading to a deeper exploration of the underlying patterns and trends.
### I have shared with you the PowerBI file, specifically named 'ExactSpace_Assessment,' in which I have extensively worked on the Data Visualization and Insights part.
"""